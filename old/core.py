import os
import shutil
import random
import numpy as np
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from scipy.signal import butter, filtfilt, lfilter, sosfiltfilt
from scipy.optimize import curve_fit
from scipy.integrate import trapz
from scipy.stats import ttest_ind
import yaml
import warnings
from tqdm import tqdm
import pathlib
from matplotlib import pyplot as plt
from matplotlib import colors
from matplotlib.lines import Line2D

from ..sync import Sync
from ..utils import *
from .plot import plot_clusters

pathlib.Path(__file__).parent.resolve()

# CONFIG_FILE_TEMPLATE = r"%s/params.yaml" % pathlib.Path(__file__).parent.resolve()
# DEFAULT_PARAMS = {}

########################
###---MAIN CLASSES---###
########################


class Rec2P:

    """
    A recording master class
    """

    def __init__(self, data_path: str, sync: Sync):

        """
        Create a Rec2P object from .npy files generated by Suite2P.

        - data_path:
            absolute path to suite2P outoput directory
        - sync:
            S object to use for aligning the stimuli to the recording
        """

        expected_files = [
            "F.npy",
            "Fneu.npy",
            "iscell.npy",
            "spks.npy",
            "stat.npy",
            "ops.npy",
        ]

        files = os.listdir(data_path)

        # check all the files are there
        for f in expected_files:

            if f not in files:

                raise Exception("%s file not found in %s" % (f, data_path))

        # load data
        print("\n> loading data from %s ..." % data_path, end=" ")

        self.data_path = data_path
        self.Fraw = np.load(data_path + r"/F.npy")
        self.Fneu = np.load(data_path + r"/Fneu.npy")
        self.iscell = np.load(data_path + r"/iscell.npy")
        self.spks = np.load(data_path + r"/spks.npy")
        self.stat = np.load(data_path + r"/stat.npy", allow_pickle=True)
        self.ops = np.load(data_path + r"/ops.npy", allow_pickle=True)

        self.nframes = self.Fraw.shape[1]
        self.ncells = self.Fraw.shape[0]

        print("OK")

        # check if the end of the last stim_window specifyed by the sync structure exceeds 
        # the length of the recording. If not, pad the recording. This can be due to premature 
        # end of the recording, where the pause after the last trial is too short.

        # if sync.sync_ds[sync.stims_names[-1]]['stim_window'][1]>self.frames:

        #     pad_len = sync.sync_ds[sync.stims_names[-1]]['stim_window'][1]-self.nframes
        #     np.pad(self.Fraw,((0,0),(0,pad_len)),mode='mean',stat_length=((0,0),(0,10)))
        #     np.pad(self.Fneu,((0,0),(0,pad_len)),mode='mean',stat_length=((0,0),(0,10)))
        #     np.pad(self.spks,((0,0),(0,pad_len)),mode='mean',stat_length=((0,0),(0,10)))

        self.data_path = data_path
        self.params_file = generate_params_file()

        self.sync = sync
        self.cells = None
        self.params = None

        # read parameters from .yaml params file
        self.load_params()

    def load_params(self):

        """
        Read parameters from .yaml params file
        """

        with open(self.params_file, "r") as f:

            self.params = yaml.load(f, Loader=yaml.Loader)

            # update also for all the cells
            if self.cells != None:

                for cell in self.cells:

                    self.cells[cell].params = self.params

            print("> parameters loaded.")

        return self.params

    def get_responsive(self):

        """
        Get a list containing the ids of all the responsive cells
        """

        ids = []

        for cell in self.cells:

            if self.cells[cell].responsive:

                ids.append(cell)

        return ids

    def get_cells(
        self, 
        keep_unresponsive: bool=False, 
        n: int = None, 
        clean_memory=True
    ):

        """
        Retrive the cells from the recording files.

        - keep_unresponsive: bool
            decide wether to keep also the cells classified as unresponsive using
            the criteria and the threshold specified in thge config file.
        - n: int
            maximum number of cell to extract. If None, all the cells will be extracted/
        - clean_memory: Bool
            if True, delete original close original .npy files
        """

        print("\n> Extracting cells ...")
        cells = {}
        responsive = []

        if n == None:

            idxs_cells = np.arange(0, self.ncells)

            if self.params["use_iscell"]:

                idxs_cells = np.where(self.iscell[:, 0] == 1)[0]
        else:

            idxs_cells = np.arange(0, n)

        for id in tqdm(idxs_cells):

            if self.params["use_iscell"]:

                # check the ROIS has been classified as cell by suite2p
                if not self.iscell[id][0]:
                    continue

            cell = Cell2P(self, id)
            cell.analyze()

            if not keep_unresponsive and not cell.responsive:

                # discard cell and free memory
                del cell

            else:

                cells |= {id: cell}

                if cell.responsive:

                    responsive.append(id)

        self.cells = cells

        if not cells:

            warnings.warn("No cells found!", RuntimeWarning)

        else:

            print(
                "> %d responsive cells found (tot: %d, keep_unresponsive: %r, use_iscell: %r)"
                % (
                    len(responsive),
                    self.ncells,
                    keep_unresponsive,
                    self.params["use_iscell"],
                )
            )

        if clean_memory:

            del self.Fraw
            del self.Fneu
            del self.spks
            del self.iscell

        return self.cells

    def compute_fingerprints(
        self, 
        stim_trials_dict=None, 
        type="dff", 
        normalize="z", 
        smooth=True
    ):

        """
        Compute a fingerprint for each cell by concatenating the average responses
        to the specified stimuli and trials.

        - stim_trials_dict: dict
            A dict which specifies which stim and which trials to concatenate for computing
            the fingerptint.
            Should contain key-values pairs such as {stim:[t1,...,tn]}, where stim is a valid
            stim name and [t1,...,tn] is a list of valid trials for that stim.
        """

        # check if the cells have already been retrived

        if self.cells == None:

            self.get_cells()

        if stim_trials_dict == None:

            stim_trials_dict = {stim: [] for stim in self.stims_trials_intersection}

        responsive = self.get_responsive()

        fingerprints = []

        for cell in responsive:

            average_resp = self.cells[cell].analyzed_trials

            # concatenate the mean responses to all the trials specified by trial_names,
            # for all the stimuli specified by stim_names.

            concat_stims = []

            for (stim, trials_names) in stim_trials_dict.items():

                if not trials_names:

                    trials_names = list(self.stims_trials_intersection[stim])

                for trial_name in trials_names:

                    r = average_resp[stim][trial_name]["average_%s" % type]

                    # cut the responses
                    start = average_resp[stim][trial_name]["window"][0]
                    stop = average_resp[stim][trial_name]["window"][1]

                    r = r[start : int(stop + start / 2)]

                    if smooth:
                        # low-pass filter
                        r = filter(r, 0.1)

                    concat_stims = np.concatenate((concat_stims, r))

            if normalize == "lin":

                concat_stims = lin_norm(concat_stims, -1, 1)

            elif normalize == "z":

                concat_stims = z_norm(concat_stims, True)

            fingerprints.append(concat_stims)

        # check lenghts consistency
        fingerprints = check_len_consistency(fingerprints)

        # convert to array
        fingerprints = np.array(fingerprints)

        ## NB: index consistency between fingerprints array and list from get_responsive() is important here!

        return fingerprints

    def get_populations(
        self,
        cells_ids=None,
        algo='pca',
        n_components=2,
        save_name='',
        **kwargs
        ):

        '''

        Find functional populations within the set of cells specified. 
        
        - cells_id: list of str
            list of valid cells ids used for identifying which subset of all the cells to analyze.
            By thefault, all the cells present in the recording will be analyzed.
        - algo: str
            algorithm for demensionality reduction. Can be pca or tsne.
        - n_components: int
            number of component used by GMM for clustering.
        - **kwargs:
            any valid argument to parametrize compute_fingerprints() method

        '''

        fp = self.compute_fingerprints(
                    cells_ids = cells_ids,
                    **kwargs)
        
        if algo=='pca':
            
            # run PCA
            transformed = PCA(n_components=2).fit_transform(fp)

        elif algo=='tsne':

            # if needed, go with Tsne
            tsne_params =  {
                    'n_components':2, 
                    'verbose':1, 
                    'metric':'cosine', 
                    'early_exaggeration':4, 
                    'perplexity':10, 
                    'n_iter':3000, 
                    'init':'pca', 
                    'angle':0.1}

            transformed = TSNE_embedding(fp,**tsne_params)

        # clusterize
        labels = GMM(transformed,n_components=n_components,covariance_type='diag')

        if save_name:
            plot_clusters(transformed,labels,algo=algo,save='%s_%s'%(save_name,algo))

        else:
            plot_clusters(transformed,labels,algo=algo,save='')

        # get popos
        pops = []
        for n in np.unique(labels):

            indices = np.where(labels == n)[0]

            c = []
            for i in indices:

                c.append(cells_ids[i])

            pops.append(c)

        return pops
    
    # OLD CODE #
    # def get_populations(
    #     self,
    #     stim_trials_dict=None,
    #     n_clusters=None,
    #     use_tsne=False,
    #     type="dff",
    #     normalize="lin",
    #     plot=True,
    # ):

    #     """
    #      Clusterize the activity traces of all the cells into population using PCA/TSNE and K-means.

    #     - stim_trials_dict: dict
    #         A dict which specifies which stim and which trials to concatenate for computing
    #         the fingerptint.
    #         Should contain key-values pairs such as {stim:[t1,...,tn]}, where stim is a valid
    #         stim name and [t1,...,tn] is a list of valid trials for that stim.
    #     - n_clusters: int
    #             Number of cluster to use for k means clustering
    #     - use_tsne: bool
    #         wether to compute tsne embedding after PCA decomposition
    #     - type: str
    #         can be either "dff" or "zspks"
    #     - normalize: str
    #         'lin': signals will be normalized between 0 and 1 before running PCA
    #         'z': signals will be normalized using z score normalization before running PCA
    #         otherwise, no normalization will be applied

    #     """
    #     responsive = self.get_responsive()

    #     # compute fingerprints
    #     x = self.compute_fingerprints(stim_trials_dict, type, normalize)

    #     # embed data
    #     if use_tsne:

    #         if len(x) < 50:
    #             n_comp = len(x)
    #         else:
    #             n_comp = 50

    #         # run PCA
    #         pca = PCA(n_components=n_comp)
    #         transformed = pca.fit_transform(x)
    #         # run t-SNE
    #         transformed = self.TSNE_embedding(x)

    #     else:

    #         # PCA embedding
    #         pca = PCA(n_components=50)
    #         transformed = pca.fit_transform(x)

    #     # if the nuber of cluster is not specified, find optimal n
    #     if n_clusters == None:

    #         n_clusters = find_optimal_kmeans_k(transformed)

    #     # run Kmeans
    #     kmeans = KMeans(n_clusters=n_clusters, init="k-means++", algorithm="auto").fit(
    #         transformed
    #     )

    #     labels = kmeans.labels_

    #     # retrive clusters
    #     clusters = []

    #     for n in np.unique(labels):

    #         indices = np.squeeze(np.argwhere(labels == n))
    #         c = []

    #         for i in indices:

    #             c.append(responsive[i])

    #         clusters.append(c)

    #     if plot:

    #         clist = list(colors.TABLEAU_COLORS.keys())
    #         markers = list(Line2D.markers.items())[2:]
    #         # random.shuffle(markers)

    #         if use_tsne:

    #             algo = "t-SNE"

    #             Xax = transformed[:, 0]
    #             Yax = transformed[:, 1]

    #             fig = plt.figure(figsize=(7, 5))
    #             ax = fig.add_subplot(111)

    #             fig.patch.set_facecolor("white")

    #             for l in np.unique(labels):

    #                 color = clist[l]
    #                 ix = np.where(labels == l)[0]

    #                 for i in ix:

    #                     marker = markers[int(responsive[i].split("_")[0])][0]
    #                     ax.scatter(
    #                         Xax[i],
    #                         Yax[i],
    #                         edgecolor=color,
    #                         s=50,
    #                         marker=marker,
    #                         facecolors="none",
    #                         alpha=0.8,
    #                     )

    #             ax.set_xlabel("%s 1" % algo, fontsize=9)
    #             ax.set_ylabel("%s 2" % algo, fontsize=9)

    #             ax.set_title("%d ROIs (n=%d)" % (len(Xax), len(self.recs)))

    #         else:

    #             algo = "PCA"

    #             Xax = transformed[:, 0]
    #             Yax = transformed[:, 1]
    #             Zax = transformed[:, 2]

    #             fig = plt.figure(figsize=(7, 5))
    #             # ax = fig.add_subplot(111, projection="3d")
    #             ax = fig.add_subplot(111)

    #             fig.patch.set_facecolor("white")

    #             for l in np.unique(labels):

    #                 color = clist[l]
    #                 ix = np.where(labels == l)[0]
    #                 # ax.scatter(
    #                 #     Xax[ix], Yax[ix], Zax[ix], c=clist[l], s=40)

    #                 for i in ix:

    #                     marker = markers[int(responsive[i].split("_")[0])][0]
    #                     ax.scatter(
    #                         Xax[i],
    #                         Yax[i],
    #                         edgecolor=color,
    #                         s=50,
    #                         marker=marker,
    #                         facecolors="none",
    #                         alpha=0.8,
    #                     )

    #             ax.set_xlabel("%s 1" % algo, fontsize=9)
    #             ax.set_ylabel("%s 2" % algo, fontsize=9)
    #             # ax.set_zlabel("%s 3"%algo, fontsize=9)

    #             ax.set_title("%d ROIs (n=%d)" % (len(Xax), len(self.recs)))

    #             # ax.view_init(30, 60)

    #     return clusters


class Cell2P:

    """
    A Cell object to process, analyze and compute statistics
    on a raw fluoressence trace extracted by suite2p from a single ROI.
    The input Rec2P object will not be copied, just referenced.

    """

    def __init__(self, rec: Rec2P, id: int):

        self.id = id
        self.label = None # usefull for pop analysis
        self.responsive = None
        self.analyzed_trials = None

        # reference data from rec object
        self.Fraw = rec.Fraw[id]
        self.Fneu = rec.Fraw[id]
        self.spks = rec.spks[id]
        self.params = rec.params

        # refrence usefull sync attibutes
        self.sync = rec.sync.sync_ds
        self.stims_names = rec.sync.stims_names
        self.trials_names = rec.sync.trials_names

        # subtract neuropil signal to raw signal
        self.FrawCorr = self.Fraw - self.Fneu * self.params["neuropil_corr"]
        # lowpass-filter raw
        self.FrawCorr = filter(self.FrawCorr, self.params["lowpass_wn"])

        # calculate dff on the whole recording
        if self.params["baseline_indices"] != None:

            self.mean_baseline = np.mean(
                self.FrawCorr[
                    self.params["baseline_indices"][0] : self.params["baseline_indices"][1]
                ]
            )

            self.dff = (self.FrawCorr - self.mean_baseline) / self.mean_baseline

            self.dff_baseline = self.dff[
                self.params["baseline_indices"][0] : self.params["baseline_indices"][1]
            ]

        else:

            self.mean_baseline = np.mean(self.FrawCorr[: rec.sync.sync_frames[0]])
            self.dff = (self.FrawCorr - self.mean_baseline) / self.mean_baseline
            self.dff_baseline = self.dff[: rec.sync.sync_frames[0]]

        # z-score and filter spikes
        self.zspks = np.where(
            abs(z_norm(self.spks)) < self.params["spks_threshold"], 0, z_norm(self.spks)
        )

    def _compute_QI_(self, trials: np.ndarray):

        """
        Calculate response quality index as defined by
        Baden et al. over a matrix with shape (reps,time).
        """
        if self.params["qi_metrics"] == 0:
                
            a = np.var(trials.mean(axis=0))
            b = np.mean(trials.var(axis=1))
            return a / b
        
        else:

            n = trials.shape[0]
            mean = np.mean(trials, axis=0)
            pre = mean[:self.params["pre_trial"]]
            post = mean[self.params["pre_trial"]:]
            pvalue = ttest_ind(pre,post)[1]
            # adjust pvalue using Sidak correction
            pvalue_corr = 1-(1-pvalue)**n
            return pvalue_corr

    def _compute_rmi_(self, a, b, mode="auc"):

        if mode == "auc":

            a_ = trapz(abs(a), dx=1)
            b_ = trapz(abs(b), dx=1)

        elif mode == "peak":

            a_ = np.max(abs(a))
            b_ = np.max(abs(b))

        rmi = (a_ - b_) / (a_ + b_)

        return rmi

    def _compute_snr_imp_(self, a, b):

        """
        Extract noise and signal components from each signal,
        compute SNR and return ration between SNRs
        """

        a_s = filter(a, 0.2, btype="low")
        a_n = filter(a, 0.2, btype="high")
        snr_a = abs(np.mean(a_s) / np.std(a_n))

        b_s = filter(b, 0.2, btype="low")
        b_n = filter(b, 0.2, btype="high")
        snr_b = abs(np.mean(b_s) / np.std(b_n))

        return snr_a / snr_b, snr_a, snr_b

    def analyze(self):

        """
        Compute average responses (df/f and z-scored spkiking activity)
        over trials for each stimulation type defined by the Sync object in rec.
        Return a dictionary with the following structure:
        {
            stim_0:{

                trial_type_0:{

                    "trials_dff",
                    "average_dff",
                    "std_dff",
                    "average_zspks",
                    "std_zspks",
                    "QI",
                    "window",
                }
                ...
            }
            ...
        }
        """

        analyzed_trials = {}
        if self.params["qi_metrics"]==0: best_qi = 0
        else: best_qi = 1

        for stim in self.stims_names:

            analyzed_trials |= {stim: {}}

            if self.params["baseline_extraction"] == 1:

                # extract only one baseline for each stimulus for computing df/f
                mean_baseline = np.mean(
                    self.FrawCorr[
                        self.sync[stim]["stim_window"][0]
                        - self.params["baseline_frames"] : self.sync[stim]["stim_window"][0]
                    ]
                )

            for trial_type in list(self.sync[stim].keys())[
                :-1
            ]:  # last item is stim_window

                trials_dff = []
                trials_spks = []
                trials_zspks = []

                trial_len = self.sync[stim][trial_type]["trial_len"]
                pause_len = self.sync[stim][trial_type]["pause_len"]

                if pause_len > self.params["max_aftertrial"]:

                    pause_len = self.params["max_aftertrial"]

                for trial in self.sync[stim][trial_type]["trials"]:

                    if self.params["baseline_extraction"] == 0:

                        # extract local baselines for each trial for computing df/f
                        mean_baseline = np.mean(
                            self.FrawCorr[
                                trial[0] - self.params["baseline_frames"] : trial[0]
                            ]
                        )

                    resp = self.FrawCorr[
                        trial[0]
                        - self.params["pre_trial"] : trial[0]
                        + trial_len
                        + pause_len
                    ]

                    resp_dff = (resp - mean_baseline) / mean_baseline

                    # smooth with lp filter
                    # resp_dff = filter(resp_dff, self.params["lowpass_wn"])

                    trials_dff.append(resp_dff)

                    # spiking activity
                    resp_spks = self.spks[
                        trial[0]
                        - self.params["pre_trial"] : trial[0]
                        + trial_len
                        + pause_len
                    ]

                    # z-scored spiking activity
                    resp_zspks = self.zspks[
                        trial[0]
                        - self.params["pre_trial"] : trial[0]
                        + trial_len
                        + pause_len
                    ]

                    trials_spks.append(resp_spks)
                    trials_zspks.append(resp_zspks)

                # convert to array
                trials_dff = np.array(trials_dff)
                trials_spks = np.array(trials_spks)
                trials_zspks = np.array(trials_zspks)

                # calculate QI over df/f traces
                # PENDING: implementation of ttest-based qi
                qi = self._compute_QI_(z_norm(filter(trials_dff, 0.3)))

                if self.params["qi_metrics"]==0:
                
                    if qi > best_qi:
                        best_qi = qi

                else:

                    if qi < best_qi:
                        best_qi = qi

                on = self.params["pre_trial"]
                off = self.params["pre_trial"] + trial_len

                on = self.params["pre_trial"]
                off = self.params["pre_trial"] + trial_len

                analyzed_trials[stim] |= {
                    trial_type: {
                        "trials_dff": trials_dff,
                        "average_dff": np.mean(trials_dff, axis=0),
                        "std_dff": np.std(trials_dff, axis=0),
                        "average_spks": np.mean(trials_spks, axis=0),
                        "std_spks": np.std(trials_spks, axis=0),
                        "average_zspks": np.mean(trials_zspks, axis=0),
                        "std_zspks": np.std(trials_zspks, axis=0),
                        "QI": qi,
                        "window": (on, off),
                    }
                }

        self.analyzed_trials = analyzed_trials
        self.qi = best_qi
        self.is_responsive()

        return analyzed_trials

    def is_responsive(self):

        """
        Asses responsiveness according to the QI value and the criteria specified in params
        """

        if self.params["resp_criteria"] == 1:

            qis_stims = []

            for stim in self.analyzed_trials:

                qis_trials = []

                for trial_name in self.analyzed_trials[stim]:

                    qis_trials.append(self.analyzed_trials[stim][trial_name]["QI"])

                # for each stimulus, consider only the highest QI calculated
                # over all the different trial types (i.e. Ipsi, Both, Contra)
                qis_stims.append(max(qis_trials))

            if self.params["qi_metrics"]==0:
                responsive = any(qi >= self.params["qi_threshold"] for qi in qis_stims)

            else:
                responsive = any(qi <= 0.05 for qi in qis_stims)

        else:

            if self.params["qi_metrics"]==0:
                responsive = (self.qi >= self.params["qi_threshold"])

            else:
                responsive = (self.qi <= 0.05)


        # # evaluate the responsiveness according to the criteria defined in the config file
        # if self.params["resp_criteria"] == 1:

        #     responsive = all(qi >= qi_threshold for qi in qis_stims)

        # else:

        #     responsive = any(qi >= qi_threshold for qi in qis_stims)

        self.responsive = responsive

        return responsive

    def calculate_modulation(self, stim, trial_name_1, trial_name_2, mode="rmi"):

        """
        Calculate Response Modulation Index on averaged responses to
        trial_type_1 vs trial_type_2 during stimulus stim.

        - stim: str
            stimulus name
        - trial_type_1: str
            first trial_type name (i.e "BOTH","CONTRA","IPSI")
        - trial_type_2: str
            second trial_type name (i.e "BOTH","CONTRA","IPSI"),
            it is supposed to be different from trial_type_1.
        """

        if not self.analyzed_trials:

            self.analyze()

        average_resp_1 = self.analyzed_trials[stim][trial_name_1]["average_dff"]
        average_resp_2 = self.analyzed_trials[stim][trial_name_2]["average_dff"]

        if mode == "rmi":

            mod = self._compute_rmi_(average_resp_1, average_resp_2)

        elif mode == "snr":

            mod = self._compute_snr_imp_(average_resp_1, average_resp_2)

        return mod

    def calculate_random_modulation(self, stim, trial_name, n_shuff=100, mode="rmi"):

        """
        Quintify the intrinsic variability in the responses generated by
        the same trial type, i.e. only due to stochastic effects, using RMI.

        - stim: str
            name of the stimulus type
        - trial_name: str
            the trial type for which the intrinsic variability is quantified.
        - n_shuff: int
            number of shuffling
        """

        shuff_mods = []

        for i in range(n_shuff):

            n_trials = len(self.sync[stim][trial_name]["trials"])

            # generate two random groups of responses generate by trial_name
            shuff_trials_idx = np.arange(n_trials)
            random.shuffle(shuff_trials_idx)

            shuff_trials_idx_a = shuff_trials_idx[: int(n_trials / 2)]
            shuff_trials_idx_b = shuff_trials_idx[int(n_trials / 2) :]

            trials_a = []

            for trial in shuff_trials_idx_a:

                trials_a.append(
                    self.analyzed_trials[stim][trial_name]["trials_dff"][trial]
                )

            trials_b = []

            for trial in shuff_trials_idx_b:

                trials_b.append(
                    self.analyzed_trials[stim][trial_name]["trials_dff"][trial]
                )

            shuff_mods.append(
                self._compute_rmi_(np.mean(trials_a, axis=0), np.mean(trials_b, axis=0))
            )

            # for trial_a, trial_b in zip(shuff_trials_idx_a,shuff_trials_idx_b):

            #     if mode=='rmi':

            #         shuff_mods.append(
            #             self._compute_rmi_(self.analyzed_trials[stim][trial_name]["trials_dff"][trial_a],
            #                                 self.analyzed_trials[stim][trial_name]["trials_dff"][trial_b])
            #         )

            #     elif mode=='snr':

            #         shuff_mods.append(
            #             self._compute_snr_imp_(self.analyzed_trials[stim][trial_name]["trials_dff"][trial_a],
            #                                 self.analyzed_trials[stim][trial_name]["trials_dff"][trial_b])
            #         )

        shuff_controls = np.mean(shuff_mods)

        return shuff_controls


class Batch2P:

    """
    Class for performing batch analysis of multiple recordings.
    All the recordings contained in a Batch2P object must share at
    least one stimulation condition with at least one common trial
    type (e.g. CONTRA, BOTH or IPSI)

    """

    def __init__(self, data_dict: dict, groups={}):

        """
        Create a Batch2P object from a data dictionary.

        - data_dict: dict
            A dictionary where each key is an absolute path to the .npy files
            of a recording, and its vlue is a Sync object generated for that recording.
            Something like {data_path_rec0:Sync_rec0,...,data_path_recN:Sync_recN}
            for a Batch2P object containing N independent recordings.
        - groups: dict
            A dictionary for assigning each recording to a group. This is useful for keeping
            joint analysis of recordings performed in different conditions (e.g. control and treated).
            The keys of the dictionary must be the same as data_dict (datapaths), and the values int numbers.
            By default, all recordings loaded are assigned to group 0.
        """

        # be sure that the sync object of all the recordings share at least 1 stimulus.
        # only shared stimuli will be used.

        self.stims_trials_intersection = {}

        stims_allrec = [sync.stims_names for sync in data_dict.values()]

        # start from minimal stim set
        stims_intersection = set(stims_allrec[np.argmin([len(s) for s in stims_allrec])])

        for stims in stims_allrec[1:]:

            stims_intersection.intersection(set(stims))

        # also, for all the shared stimuli,
        # select only trials type are shared for that specific stimulus by all recs.

        for stim in stims_intersection:

            # start with trials for stimulus "stim" in first sync object
            all_trials = list(list(data_dict.values())[0].sync_ds[stim].keys())[:-1]

            trials_intersection = set(all_trials)

            for sync in list(data_dict.values())[1:]:

                # last item is "window_len"
                trials_intersection.intersection(
                    set(list(sync.sync_ds[stim].keys())[:-1])
                )

            self.stims_trials_intersection |= {stim: list(trials_intersection)}

        # generate params.yaml
        generate_params_file()

        # instantiate the Rec2P objects
        self.recs = {}
        self.groups = groups

        for rec_id, (data_path, sync) in enumerate(data_dict.items()):

            if data_path not in groups:
                group_id = 0

            else:
                group_id = groups[data_path]

            rec = Rec2P(data_path, sync)
            self.recs |= {rec_id: (rec, group_id)}

        self.cells = None

    def load_params(self):

        """
        Read parameters from .yaml params file

        """

        for rec in self.recs:

            rec.load_params()

    def get_cells(self):

        """
        Extract all the cells from the individual recordings and assign new ids.
        Id is in the form G_R_C, where G,R and C are int which specify the group,
        the recording and the cell ids.

        """

        self.cells = {}

        for (rec_id, value) in self.recs.items():

            rec = value[0]
            group_id = value[1]

            # retrive cells for each recording
            rec.get_cells()

            for (cell_id, cell) in rec.cells.items():

                # new id
                new_id = "%s_%s_%s" % (str(group_id), str(rec_id), str(cell_id))
                self.cells |= {new_id: cell}
                # update cell id
                cell.id = new_id

        # RETRIVE THE GROUPS
        self.cells_groups = {g:{} for g in set(self.groups.values())}

        for id,cell in self.cells.items():
            for g in self.cells_groups:

                if int(id.split('_')[0])==g:

                    self.cells_groups[g] |= {id:cell}

        return self.cells

    def get_responsive(self):

        """
        Get a list containing the ids of all the responsive cells

        """

        ids = []

        for cell in self.cells:

            if self.cells[cell].responsive:

                ids.append(cell)

        return ids

    def compute_fingerprints(
        self, 
        cells_ids=None,
        stim_trials_dict=None, 
        type="dff", 
        normalize="z", 
        smooth=True
    ):

        """
        Compute a fingerprint for each cell by concatenating the average responses
        to the specified stimuli and trials.

        - cells_ids: list of valid ids
            by default, compute fingerprints of all the responsive cells
        - stim_trials_dict: dict
            A dict which specifies which stim and which trials to concatenate for computing
            the fingerptint.
            Should contain key-values pairs such as {stim:[t1,...,tn]}, where stim is a valid
            stim name and [t1,...,tn] is a list of valid trials for that stim.
        """

        # check if the cells have already been retrived

        if self.cells == None:

            self.get_cells()

        if stim_trials_dict == None:

            stim_trials_dict = {stim: [] for stim in self.stims_trials_intersection}

        responsive = self.get_responsive()

        fingerprints = []

        if cells_ids == None:
            
            cells_ids = responsive

        for cell in cells_ids:

            average_resp = self.cells[cell].analyzed_trials

            # concatenate the mean responses to all the trials specified by trial_names,
            # for all the stimuli specified by stim_names.

            concat_stims = []

            for (stim, trials_names) in stim_trials_dict.items():

                if not trials_names:

                    trials_names = list(self.stims_trials_intersection[stim])

                for trial_name in trials_names:

                    r = average_resp[stim][trial_name]["average_%s" % type]

                    # cut the responses
                    start = average_resp[stim][trial_name]["window"][0]
                    stop = average_resp[stim][trial_name]["window"][1]

                    r = r[start : int(stop + start / 2)]

                    if smooth:
                        # low-pass filter
                        r = filter(r, 0.3)

                    concat_stims = np.concatenate((concat_stims, r))

            if normalize == "lin":

                concat_stims = lin_norm(concat_stims, -1, 1)

            elif normalize == "z":

                concat_stims = z_norm(concat_stims, True)

            fingerprints.append(concat_stims)

        # check lenghts consistency
        fingerprints = check_len_consistency(fingerprints)

        # convert to array
        fingerprints = np.array(fingerprints)

        ## NB: index consistency between fingerprints array and list from get_responsive() is important here!

        return fingerprints

    def get_populations(
        self,
        cells_ids=None,
        algo='pca',
        markers=True,
        save_name='',
        groups_name=None,
        n_components=2,
        **kwargs
        ):

        '''

        Find functional populations within the set of cells specified. 
        
        - cells_id: list of str
            list of valid cells ids used for identify which subset of all the cells to analyze.
            By thefault, all the cells present in the batch will be analyzed.
        - algo: str
            algorithm for demensionality reduction. Can be pca or tsne.
        - n_components: int
            number of component used by GMM for clustering.
        - **kwargs:
            any valid argument to parametrize compute_fingerprints() method

        '''

        fp = self.compute_fingerprints(
                    cells_ids = cells_ids,
                    **kwargs)
        
        if algo=='pca':
            
            # run PCA
            transformed = PCA(n_components=2).fit_transform(fp)

        elif algo=='tsne':

            # if needed, go with Tsne
            tsne_params =  {
                    'n_components':2, 
                    'verbose':1, 
                    'metric':'cosine', 
                    'early_exaggeration':4, 
                    'perplexity':10, 
                    'n_iter':3000, 
                    'init':'pca', 
                    'angle':0.1}

            transformed = TSNE_embedding(fp,**tsne_params)

        # clusterize
        labels = GMM(transformed,n_components=n_components,covariance_type='diag')

        if markers:
            markers = [int(id.split(sep='_')[0]) for id in cells_ids]

        else:
            markers=None

        if save_name:
            plot_clusters(transformed,labels,markers,groups_name=groups_name,algo=algo,save='%s_%s'%(save_name,algo))

        else:
            plot_clusters(transformed,labels,markers,algo=algo,save='')

        # get popos
        pops = []
        for n in np.unique(labels):

            indices = np.where(labels == n)[0]

            c = []
            for i in indices:

                c.append(cells_ids[i])

            pops.append(c)

        return pops
    

    # OLD CODE #
    # def get_populations(
    #     self,
    #     stim_trials_dict=None,
    #     n_clusters=None,
    #     use_tsne=False,
    #     type="dff",
    #     normalize="lin",
    #     plot=True,
    # ):

    #     """
    #      Clusterize the activity traces of all the cells into population using PCA/TSNE and K-means.

    #      - stim_trials_dict: dict
    #          A dict which specifies which stim and which trials to concatenate for computing
    #          the fingerptint.
    #          Should contain key-values pairs such as {stim:[t1,...,tn]}, where stim is a valid
    #          stim name and [t1,...,tn] is a list of valid trials for that stim.
    #     - n_clusters: int
    #          Number of cluster to use for k means clustering
    #      - use_tsne: bool
    #          wether to compute tsne embedding after PCA decomposition
    #      - type: str
    #          can be either "dff" or "zspks"
    #      - normalize: str
    #          'lin': signals will be normalized between 0 and 1 before running PCA
    #          'z': signals will be normalized using z score normalization before running PCA
    #          otherwise, no normalization will be applied

    #     """
    #     responsive = self.get_responsive()

    #     # compute fingerprints
    #     x = self.compute_fingerprints(stim_trials_dict, type, normalize)

    #     # embed data
    #     if use_tsne:

    #         if len(x) < 50:
    #             n_comp = len(x)
    #         else:
    #             n_comp = 50

    #         # run PCA
    #         pca = PCA(n_components=n_comp)
    #         transformed = pca.fit_transform(x)
    #         # run t-SNE
    #         transformed = self.TSNE_embedding(x)

    #     else:

    #         # PCA embedding
    #         pca = PCA(n_components=50)
    #         transformed = pca.fit_transform(x)

    #     # if the nuber of cluster is not specified, find optimal n
    #     if n_clusters == None:

    #         n_clusters = find_optimal_kmeans_k(transformed)

    #     # run Kmeans
    #     kmeans = KMeans(n_clusters=n_clusters, init="k-means++", algorithm="auto").fit(
    #         transformed
    #     )

    #     labels = kmeans.labels_

    #     # retrive clusters
    #     clusters = []

    #     for n in np.unique(labels):

    #         indices = np.squeeze(np.argwhere(labels == n))
    #         c = []

    #         for i in indices:

    #             c.append(responsive[i])

    #         clusters.append(c)

    #     if plot:

    #         clist = list(colors.TABLEAU_COLORS.keys())
    #         markers = list(Line2D.markers.items())[2:]
    #         # random.shuffle(markers)

    #         if use_tsne:

    #             algo = "t-SNE"

    #             Xax = transformed[:, 0]
    #             Yax = transformed[:, 1]

    #             fig = plt.figure(figsize=(7, 5))
    #             ax = fig.add_subplot(111)

    #             fig.patch.set_facecolor("white")

    #             for l in np.unique(labels):

    #                 color = clist[l]
    #                 ix = np.where(labels == l)[0]

    #                 for i in ix:

    #                     marker = markers[int(responsive[i].split("_")[0])][0]
    #                     ax.scatter(
    #                         Xax[i],
    #                         Yax[i],
    #                         edgecolor=color,
    #                         s=50,
    #                         marker=marker,
    #                         facecolors="none",
    #                         alpha=0.8,
    #                     )

    #             ax.set_xlabel("%s 1" % algo, fontsize=9)
    #             ax.set_ylabel("%s 2" % algo, fontsize=9)

    #             ax.set_title("%d ROIs (n=%d)" % (len(Xax), len(self.recs)))

    #         else:

    #             algo = "PCA"

    #             Xax = transformed[:, 0]
    #             Yax = transformed[:, 1]
    #             Zax = transformed[:, 2]

    #             fig = plt.figure(figsize=(7, 5))
    #             # ax = fig.add_subplot(111, projection="3d")
    #             ax = fig.add_subplot(111)

    #             fig.patch.set_facecolor("white")

    #             for l in np.unique(labels):

    #                 color = clist[l]
    #                 ix = np.where(labels == l)[0]
    #                 # ax.scatter(
    #                 #     Xax[ix], Yax[ix], Zax[ix], c=clist[l], s=40)

    #                 for i in ix:

    #                     marker = markers[int(responsive[i].split("_")[0])][0]
    #                     ax.scatter(
    #                         Xax[i],
    #                         Yax[i],
    #                         edgecolor=color,
    #                         s=50,
    #                         marker=marker,
    #                         facecolors="none",
    #                         alpha=0.8,
    #                     )

    #             ax.set_xlabel("%s 1" % algo, fontsize=9)
    #             ax.set_ylabel("%s 2" % algo, fontsize=9)
    #             # ax.set_zlabel("%s 3"%algo, fontsize=9)

    #             ax.set_title("%d ROIs (n=%d)" % (len(Xax), len(self.recs)))

    #             # ax.view_init(30, 60)

    #     return clusters

